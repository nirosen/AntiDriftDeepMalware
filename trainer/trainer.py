import torch
from abc import abstractmethod
from numpy import inf
from logger import TensorboardWriter


import numpy as np
import torch


from tqdm import tqdm
import math
from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau
import pandas as pd
from sklearn.metrics import roc_auc_score
from utils import mkdir, get_logger 
import os

def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']


def train_step(epoch, model, criterion, metric_ftns, optimizer, lr_scheduler, device, train_loader):
    logger = get_logger('train')
    desc = ("Training train set - epoch %i" % (epoch))
    logger.info(desc)
    # Turn on training mode which enables dropout.
    model.train()
    # train_metrics.reset()
    running_loss = 0
    with tqdm(range(len(train_loader)), desc=desc, total=(len(train_loader))) as t:
        # do batches
        for batch_idx, batch in zip(t, train_loader):
            padded_sequence_tensors, labels, seq_lengths, _, samples_times, samples_hashes = batch  # batch return padded_sequence_tensor, syscalls_label, seq_length, sample_weight, self.times[item], self.hashes[item]
            labels = torch.Tensor(labels).to(device).long()
            # seq_lengths = torch.Tensor(seq_lengths).to(device).long()  # TODO: pytorch-1.7 (>1.4) bug - 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
            padded_sequence_tensors = torch.stack(padded_sequence_tensors).to(device)
            # forward the packed sequences batch and get the predicted log probabilities
            predicted_log_probs = model(padded_sequence_tensors, seq_lengths)  # , hn)
            pred_loss = criterion(predicted_log_probs, labels)
            optimizer.zero_grad()
            if not math.isfinite(pred_loss):  # isfinite isnan
                logger.info("Loss is {}, resetting loss and skipping training iteration".format(pred_loss))
                logger.info('Loss values were: ', pred_loss)
                pred_loss = torch.tensor(0)  # pred_loss = {k: torch.tensor(0) for k, v in pred_loss.items()}
            else:
                # TODO: If ``False``, the graph used to compute
                # the grads will be freed. Note that in nearly all cases setting
                # this option to True is not needed and often can be worked around
                # in a much more efficient way. Defaults to the value of `create_graph`.
                pred_loss.backward(retain_graph=True)  # added retain_graph=True after BiGru
                optimizer.step()
                if isinstance(lr_scheduler, CyclicLR):
                    logger.info("LR scheduler is of type CyclicLR, doing step.")
                    lr_scheduler.step()
                    lr = get_lr(optimizer)
                    logger.info("new lr is %.2f." %(lr))
            # if warmup_scheduler is not None:
                # warmup_scheduler.step()
            scalar_loss = pred_loss.item()  # correct = predicted_log_probs.argmax(dim=1).eq(labels).sum().item()
            running_loss += scalar_loss
            avg_loss = running_loss / (batch_idx + 1)
            # acc = metric(F.softmax(predicted_log_probs, dim=1), labels)
            t.set_postfix(loss=avg_loss)
            padded_sequence_tensors = labels = seq_lengths = predicted_log_probs = pred_loss = None
            # if batch % args.log_interval == 0 and batch > 0:
            #     cur_loss = total_loss / args.log_interval
            #     elapsed = time.time() - start_time
            #     print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '
            #             'loss {:5.2f} | ppl {:8.2f}'.format(
            #         epoch, batch, len(train_data) // args.bptt, lr,
            #         elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))
            #     total_loss = 0
            #     start_time = time.time()



def eval_step(epoch, model, criterion, metric_ftns, optimizer, lr_scheduler, device, eval_loader, set_name):
    logger = get_logger('train')
    desc = ("Evaluating %s set - epoch %i" % (set_name, epoch))
    logger.info(desc)
    # Turn on training mode which enables dropout.
    model.eval()
    # train_metrics.reset()
    running_loss, running_log_preds, running_labels, running_times, running_hashes = 0, [], [], [], []
    with torch.no_grad():
        with tqdm(range(len(eval_loader)), desc=desc, total=(len(eval_loader))) as t:
            # do batches
            for batch_idx, batch in zip(t, eval_loader):
                # update prog bar
                padded_sequence_tensors, samples_labels, seq_lengths, _, samples_times, samples_hashes = batch  # batch return padded_sequence_tensor, syscalls_label, seq_length, sample_weight, self.times[item], self.hashes[item]
                labels = torch.Tensor(samples_labels).to(device).long()
                # seq_lengths = torch.Tensor(seq_lengths).to(device).long()  # shuould be on cpu from pytorch-1.7 (>1.4) bug - 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
                padded_sequence_tensors = torch.stack(padded_sequence_tensors).to(device)

                # forward the packed sequences batch and get the predicted log probabilities
                predicted_log_probs = model(padded_sequence_tensors, seq_lengths)  # , hn)
                pred_loss = criterion(predicted_log_probs, labels)
                
                # TODO: do this in eval as well?
                # if not math.isfinite(pred_loss):  # isfinite isnan
                #     print("Loss is {}, resetting loss and skipping eval iteration".format(pred_loss))
                #     print('Loss values were: ', pred_loss)
                #     pred_loss = torch.tensor(0)  # pred_loss = {k: torch.tensor(0) for k, v in pred_loss.items()}
                # else:
                #     # TODO: If ``False``, the graph used to compute
                #     # the grads will be freed. Note that in nearly all cases setting
                #     # this option to True is not needed and often can be worked around
                #     # in a much more efficient way. Defaults to the value of `create_graph`.
                #     pred_loss.backward(retain_graph=True)  # added retain_graph=True after BiGru
                #     optimizer.step()

                scalar_loss = pred_loss.item()  # correct = predicted_log_probs.argmax(dim=1).eq(labels).sum().item()
                running_loss += scalar_loss
                avg_loss = running_loss / (batch_idx + 1)
                # acc = metric(F.softmax(predicted_log_probs, dim=1), labels)

                # save batch stats
                running_log_preds+=predicted_log_probs.tolist()
                running_labels+=samples_labels
                running_times+=samples_times
                running_hashes+=samples_hashes
                
                t.set_postfix(loss=avg_loss)
                padded_sequence_tensors = labels = seq_lengths = predicted_log_probs = pred_loss = None

                # if batch % args.log_interval == 0 and batch > 0:
                #     cur_loss = total_loss / args.log_interval
                #     elapsed = time.time() - start_time
                #     print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '
                #             'loss {:5.2f} | ppl {:8.2f}'.format(
                #         epoch, batch, len(train_data) // args.bptt, lr,
                #         elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))
                #     total_loss = 0
                #     start_time = time.time()
    epoch_stats_df = pd.DataFrame()
    # running_preds = np.exp(running_log_preds.cpu()) 
    # epoch_stats_df['predicted_probability'] = running_preds
    epoch_stats_df['predicted_log_probability'] = running_log_preds
    epoch_stats_df['label'] = running_labels
    epoch_stats_df['time'] = running_times
    epoch_stats_df['hash'] = running_hashes
    return avg_loss, epoch_stats_df

def load_opt_sched(filename):
    checkpoint = torch.load(filename) 
    return checkpoint['optimizer_state_dict'], checkpoint['lr_scheduler_state_dict']
    # load architecture params from checkpoint.
    # if checkpoint['arch'] != self.config['arch']:
        # self.logger.warning("Warning: Architecture configuration given in config file is different from that of "
                            # "checkpoint. This may yield an exception while state_dict is being loaded.")
    # load architecture params from checkpoint.
    # if checkpoint['arch'] != self.config['arch']:
        # self.logger.warning("Warning: Architecture configuration given in config file is different from that of "
    #                         # "checkpoint. This may yield an exception while state_dict is being loaded.")
    # # load optimizer state from checkpoint only when optimizer type is not changed.
    # if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:
    #     self.logger.warning("Warning: Optimizer type given in config file is different from that of checkpoint. "
    #                         "Optimizer parameters not being resumed.")
    
def save_checkpoint(checkpoints_dir, epoch, architecture, model=None, optimizer=None, lr_scheduler=None):
    state = {}
    if model and optimizer and lr_scheduler:
        state = {
            'architecture': architecture,
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'lr_scheduler_state_dict': lr_scheduler.state_dict()        
            # 'monitor_best': self.mnt_best,
            # 'config': self.config
        }
    elif optimizer and lr_scheduler:
        state = {
            'architecture': architecture,
            'epoch': epoch,
            'optimizer_state_dict': optimizer.state_dict(),
            'lr_scheduler_state_dict': lr_scheduler.state_dict()        
            # 'monitor_best': self.mnt_best,
            # 'config': self.config
        }
    else:
        return
    filename = os.path.join(checkpoints_dir, 'checkpoint-epoch-'+str(epoch)+'.pt')
    torch.save(state, filename)
    # self.logger.info("Saving checkpoint: {} ...".format(filename))
    # if save_best:
    #     best_path = str(self.checkpoint_dir / 'model_best.pt')
    #     torch.save(state, best_path)
    #     self.logger.info("Saving current best: model_best.pt ...")

def save_epoch_stats(epochs_stats_dir, epoch, epoch_stats_df):
    epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch'+str(epoch)+'_stats_df_pickle.gz'))
    # epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch'+str(epoch)+'_stats_df.gzip'), compression='gzip')
    # epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch'+str(epoch)+'_stats_df.bz2'), compression='bz2')
    # epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch'+str(epoch)+'_stats_df.zip'), compression='zip')
    # epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch'+str(epoch)+'_stats_df.xz'), compression='xz')


"""
Full training logic
"""
# Full training logic
def train(args, model, criterion, metric_ftns, optimizer, lr_scheduler, device, train_loader, val_loader=None, test_loader=None, future_loader=None, len_epoch=None):
    logger = get_logger('train')
    epochs_stats_dir = os.path.join(args.output_dir, 'epochs_stats')
    mkdir(epochs_stats_dir)
    if args.save_checkpoints != 'none':
        checkpoints_dir = os.path.join(args.output_dir, 'checkpoints')
        mkdir(checkpoints_dir)

    # not_improved_count = 0

    # Training logic for an epoch
    for epoch in range(1, args.epochs + 1):
        # do train step for this epoch
        train_step(epoch, model, criterion, metric_ftns, optimizer, lr_scheduler, device, train_loader)
        
        # do evaluation of each set    # if epoch % args.eval_every == (args.eval_every - 1):
        for eval_loader, set_name in zip([train_loader, val_loader, test_loader, future_loader], ['train', 'valid', 'test', 'future']):
            # skip None sets for retrain/double-retrain procedures
            if eval_loader:
                eval_loss, epoch_stats_df = eval_step(epoch, model, criterion, metric_ftns, optimizer, lr_scheduler, device, eval_loader, set_name)
                # TODO: save epoch_stats_df to outputdir!!!
                save_epoch_stats(epochs_stats_dir, epoch, epoch_stats_df)
                auc = roc_auc_score(epoch_stats_df['label'], epoch_stats_df.predicted_log_probability.map(lambda x: x[1]))
                logger.info("finished eval of epoch-%d of %s-set \t auc=%.2f" %(epoch, set_name, auc))
                # Do LR-step (from type ReduceLROnPlateau) and early-stopping according to validation-set.
                if set_name == 'valid':
                    # double-retrain needs to do set the same LR as retrain checkpoints. 
                    if isinstance(lr_scheduler, ReduceLROnPlateau):
                        logger.info("LR scheduler is of type ReduceLROnPlateau, doing step(auc).")
                        lr_scheduler.step(auc)
                        lr = get_lr(optimizer)
                        logger.info("new lr is %.10f." %(lr))
                if set_name == 'train' and args.train_procedure == 'double_retrain':
                    filename = os.path.join(args.output_dir, "..", "retrain", 'checkpoints', 'checkpoint-epoch-'+str(epoch)+'.pt')
                    if os.path.exists(filename):
                        loaded = load_opt_sched(filename)
                        if loaded:
                            optimizer.load_state_dict(loaded[0])
                            lr_scheduler.load_state_dict(loaded[1])

                    # best_loss, stop_step, stop = early_stopping(eval_loss, best_loss, stop_step, args.patience)
                    # if stop:
                    #     break
                    # if (stop_step == 0) & (args.save_results):
                    #     best_epoch = epoch
                    #     torch.save(model.state_dict(), model_weights / (model_name + ".pt"))
        
        # save epoch_stats_df & model
        if args.save_checkpoints != 'none':
            saved_model, saved_optimizer, saved_lr_scheduler = None, None, None
            if args.save_checkpoints == 'all':
                saved_model = model
                saved_optimizer = optimizer
                saved_lr_scheduler = lr_scheduler
            if args.save_checkpoints == 'lr':
                saved_model = None
                saved_optimizer = optimizer
                saved_lr_scheduler = lr_scheduler
            save_checkpoint(checkpoints_dir, epoch, args.architecture, saved_model, saved_optimizer, saved_lr_scheduler)
            



# def calc_metrics(epoch_stats_df):
#     metrics = {}
#     y_true = epoch_stats_df['label']
#     y_predicted = epoch_stats_df['predicted_log_probability'].argmax(dim=1).numpy()
#     # True positive
#     tp = np.sum(y_true * y_predicted)
#     # False positive
#     fp = np.sum((y_true == 0) * y_predicted)
#     # True negative
#     tn = np.sum((y_true == 0) * (y_predicted == 0))
#     # False negative
#     fn = np.sum(y_true * (y_predicted == 0))
#     # True positive rate (sensitivity or recall)
#     tpr = tp / (tp + fn)
#     # False positive rate (fall-out)
#     fpr = fp / (fp + tn)
#     # Precision
#     precision = tp / (tp + fp)
#     # True negatvie tate (specificity)
#     tnr = 1 - fpr
#     # F1 score
#     #f1 = 2 * tp / (2 * tp + fp + fn)
#     # ROC-AUC for binary classification
#     auc = (tpr + tnr) / 2





# def _save_checkpoint(self, epoch, save_best=False):
#     """
#     Saving checkpoints

#     :param epoch: current epoch number
#     :param log: logging information of the epoch
#     :param save_best: if True, rename the saved checkpoint to 'model_best.pt'
#     """
#     arch = type(self.model).__name__
#     state = {
#         'arch': arch,
#         'epoch': epoch,
#         'state_dict': self.model.state_dict(),
#         'optimizer': self.optimizer.state_dict(),
#         'monitor_best': self.mnt_best,
#         'config': self.config
#     }
#     filename = str(self.checkpoint_dir / 'checkpoint-epoch{}.pt'.format(epoch))
#     torch.save(state, filename)
#     self.logger.info("Saving checkpoint: {} ...".format(filename))
#     if save_best:
#         best_path = str(self.checkpoint_dir / 'model_best.pt')
#         torch.save(state, best_path)
#         self.logger.info("Saving current best: model_best.pt ...")

# def _resume_checkpoint(self, resume_path):
#     """
#     Resume from saved checkpoints

#     :param resume_path: Checkpoint path to be resumed
#     """
#     resume_path = str(resume_path)
#     self.logger.info("Loading checkpoint: {} ...".format(resume_path))
#     checkpoint = torch.load(resume_path)
#     self.start_epoch = checkpoint['epoch'] + 1
#     self.mnt_best = checkpoint['monitor_best']

#     # load architecture params from checkpoint.
#     if checkpoint['config']['arch'] != self.config['arch']:
#         self.logger.warning("Warning: Architecture configuration given in config file is different from that of "
#                             "checkpoint. This may yield an exception while state_dict is being loaded.")
#     self.model.load_state_dict(checkpoint['state_dict'])

#     # load optimizer state from checkpoint only when optimizer type is not changed.
#     if checkpoint['config']['optimizer']['type'] != self.config['optimizer']['type']:
#         self.logger.warning("Warning: Optimizer type given in config file is different from that of checkpoint. "
#                             "Optimizer parameters not being resumed.")
#     else:
#         self.optimizer.load_state_dict(checkpoint['optimizer'])

#     self.logger.info("Checkpoint loaded. Resume training from epoch {}".format(self.start_epoch))

    







# def template_valid_epoch(self, epoch):
#     """
#     Validate after training an epoch

#     :param epoch: Integer, current training epoch.
#     :return: A log that contains information about validation
#     """
#     self.model.eval()
#     self.valid_metrics.reset()
#     with torch.no_grad():
#         for batch_idx, (data, target) in enumerate(self.valid_data_loader):
#             data, target = data.to(self.device), target.to(self.device)

#             output = self.model(data)
#             loss = self.criterion(output, target)

#             self.writer.set_step((epoch - 1) * len(self.valid_data_loader) + batch_idx, 'valid')
#             self.valid_metrics.update('loss', loss.item())
#             for met in self.metric_ftns:
#                 self.valid_metrics.update(met.__name__, met(output, target))
#             self.writer.add_image('input', make_grid(data.cpu(), nrow=8, normalize=True))

#     # add histogram of model parameters to the tensorboard
#     for name, p in self.model.named_parameters():
#         self.writer.add_histogram(name, p, bins='auto')
#     return self.valid_metrics.result()

# def template_progress(self, batch_idx):
#     base = '[{}/{} ({:.0f}%)]'
#     if hasattr(self.data_loader, 'n_samples'):
#         current = batch_idx * self.data_loader.batch_size
#         total = self.data_loader.n_samples
#     else:
#         current = batch_idx
#         total = self.len_epoch
#     return base.format(current, total, 100.0 * current / total)






    #         self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx)
    #         self.train_metrics.update('loss', loss.item())
    #         for met in self.metric_ftns:
    #             self.train_metrics.update(met.__name__, met(output, target))

    #         if batch_idx % self.log_step == 0:
    #             self.logger.debug('Train Epoch: {} {} Loss: {:.6f}'.format(
    #                 epoch,
    #                 self._progress(batch_idx),
    #                 loss.item()))
    #             self.writer.add_image('input', make_grid(data.cpu(), nrow=8, normalize=True))

    #         if batch_idx == self.len_epoch:
    #             break
    #     log = self.train_metrics.result()

    #     # do valid
    #     if self.do_validation:
    #         val_log = self._valid_epoch(epoch)
    #         log.update(**{'val_'+k : v for k, v in val_log.items()})

    #     # do lr-step
    #     if self.lr_scheduler is not None:
    #         self.lr_scheduler.step()


    # # save logged informations into log dict
    # log = {'epoch': epoch}
    # log.update(result)

    # # print logged informations to the screen
    # for key, value in log.items():
    #     self.logger.info('    {:15s}: {}'.format(str(key), value))

    # # evaluate model performance according to configured metric, save best checkpoint as model_best
    # best = False
    # if self.mnt_mode != 'off':
    #     try:
    #         # check whether model performance improved or not, according to specified metric(mnt_metric)
    #         improved = (self.mnt_mode == 'min' and log[self.mnt_metric] <= self.mnt_best) or \
    #                     (self.mnt_mode == 'max' and log[self.mnt_metric] >= self.mnt_best)
    #     except KeyError:
    #         self.logger.warning("Warning: Metric '{}' is not found. "
    #                             "Model performance monitoring is disabled.".format(self.mnt_metric))
    #         self.mnt_mode = 'off'
    #         improved = False

    #     if improved:
    #         self.mnt_best = log[self.mnt_metric]
    #         not_improved_count = 0
    #         best = True
    #     else:
    #         not_improved_count += 1

    #     if not_improved_count > self.early_stop:
    #         self.logger.info("Validation performance didn\'t improve for {} epochs. "
    #                             "Training stops.".format(self.early_stop))

    # # save-checkpoint
    # if epoch % self.save_period == 0:
    #     self._save_checkpoint(epoch, save_best=best)













# import numpy as np
# import torch
# # from torchvision.utils import make_grid
# from base import BaseTrainer
# from utils import inf_loop, MetricTracker


# class Trainer(BaseTrainer):
#     """
#     Trainer class
#     """
#     def __init__(self, model, criterion, metric_ftns, optimizer, config, device,
#                  data_loader, valid_data_loader=None, lr_scheduler=None, len_epoch=None):
#         super().__init__(model, criterion, metric_ftns, optimizer, config)
#         self.config = config
#         self.device = device
#         self.data_loader = data_loader
#         if len_epoch is None:
#             # epoch-based training
#             self.len_epoch = len(self.data_loader)
#         else:
#             # iteration-based training
#             self.data_loader = inf_loop(data_loader)
#             self.len_epoch = len_epoch
#         self.valid_data_loader = valid_data_loader
#         self.do_validation = self.valid_data_loader is not None
#         self.lr_scheduler = lr_scheduler
#         self.log_step = int(np.sqrt(data_loader.batch_size))

#         self.train_metrics = MetricTracker('loss', *[m.__name__ for m in self.metric_ftns], writer=self.writer)
#         self.valid_metrics = MetricTracker('loss', *[m.__name__ for m in self.metric_ftns], writer=self.writer)

#     def _train_epoch(self, epoch):
#         """
#         Training logic for an epoch

#         :param epoch: Integer, current training epoch.
#         :return: A log that contains average loss and metric in this epoch.
#         """
#         self.model.train()
#         self.train_metrics.reset()
#         for batch_idx, (data, target) in enumerate(self.data_loader):
#             data, target = data.to(self.device), target.to(self.device)

#             self.optimizer.zero_grad()
#             output = self.model(data)
#             loss = self.criterion(output, target)
#             loss.backward()
#             self.optimizer.step()

#             self.writer.set_step((epoch - 1) * self.len_epoch + batch_idx)
#             self.train_metrics.update('loss', loss.item())
#             for met in self.metric_ftns:
#                 self.train_metrics.update(met.__name__, met(output, target))

#             if batch_idx % self.log_step == 0:
#                 self.logger.debug('Train Epoch: {} {} Loss: {:.6f}'.format(
#                     epoch,
#                     self._progress(batch_idx),
#                     loss.item()))
#                 self.writer.add_image('input', make_grid(data.cpu(), nrow=8, normalize=True))

#             if batch_idx == self.len_epoch:
#                 break
#         log = self.train_metrics.result()

#         if self.do_validation:
#             val_log = self._valid_epoch(epoch)
#             log.update(**{'val_'+k : v for k, v in val_log.items()})

#         if self.lr_scheduler is not None:
#             self.lr_scheduler.step()
#         return log

#     def _valid_epoch(self, epoch):
#         """
#         Validate after training an epoch

#         :param epoch: Integer, current training epoch.
#         :return: A log that contains information about validation
#         """
#         self.model.eval()
#         self.valid_metrics.reset()
#         with torch.no_grad():
#             for batch_idx, (data, target) in enumerate(self.valid_data_loader):
#                 data, target = data.to(self.device), target.to(self.device)

#                 output = self.model(data)
#                 loss = self.criterion(output, target)

#                 self.writer.set_step((epoch - 1) * len(self.valid_data_loader) + batch_idx, 'valid')
#                 self.valid_metrics.update('loss', loss.item())
#                 for met in self.metric_ftns:
#                     self.valid_metrics.update(met.__name__, met(output, target))
#                 self.writer.add_image('input', make_grid(data.cpu(), nrow=8, normalize=True))

#         # add histogram of model parameters to the tensorboard
#         for name, p in self.model.named_parameters():
#             self.writer.add_histogram(name, p, bins='auto')
#         return self.valid_metrics.result()

#     def _progress(self, batch_idx):
#         base = '[{}/{} ({:.0f}%)]'
#         if hasattr(self.data_loader, 'n_samples'):
#             current = batch_idx * self.data_loader.batch_size
#             total = self.data_loader.n_samples
#         else:
#             current = batch_idx
#             total = self.len_epoch
#         return base.format(current, total, 100.0 * current / total)
