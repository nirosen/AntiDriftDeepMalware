# template imports
import argparse
import collections
import torch
import numpy as np
# import data_loader.data_loaders as module_data
import model.loss as module_loss
import model.metric as module_metric
import model.model as module_arch
from parse_config import ConfigParser
from trainer import Trainer
from utils import prepare_device, mkdir, get_logger 
from data_loader.data_loaders import get_data_loaders

# fix random seeds for reproducibility
SEED = 123
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(SEED)


import logging



# external imports
import os
import time
from datetime import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter, summary # pytorch TB logger
# from ignite.contrib.handlers import TensorboardLogger  # ignite TB logger
# from ignite.contrib.handlers.tensorboard_logger import WeightsScalarHandler, GradsScalarHandler  # TODO: these are optional only - heavy ones - GradsHistHandler, WeightsHistHandler
# from ignite.engine import Events
# from ignite.metrics import RunningAverage  # Accuracy, Loss, Precision, Recall  # from ignite.contrib.metrics import ROC_AUC, RocCurve  # from custom_metrics import *  # TrimmedAUC, MaxTPR, RocCurvePlot, RocCurveIterationsPlot
# from ignite.handlers import ModelCheckpoint, Checkpoint, EarlyStopping, global_step_from_engine
# from ignite.contrib.handlers import ProgressBar
# import optuna
import joblib
from copy import deepcopy
# import matplotlib.pyplot as plt
# from tensorboard.compat.proto.summary_pb2 import Summary
# from tensorboard.compat.proto.tensor_pb2 import TensorProto
# from tensorboard.compat.proto.tensor_shape_pb2 import TensorShapeProto
from glob import glob

# local imports
# import models
# from engines import create_trainer, create_evaluator
# from utilities import *

# globals
task_time = datetime.now().strftime("%d%m%Y_%H%M%S")
output_dir = os.path.join('..','output','output_'+task_time)  # '/home/john/Desktop/git/scnn_ignite/output/output_'+task_time

# num_classes is good/bad
# categories_count is the number of uniq syscalls
# TODO: change categories_count to 46 for 60K_Max1000 or 48 without Max1000
configuration_data = {'num_classes': 2, 'categories_count': 46, 'embed_size': 300, 'hidden_size': 300}  # TODO: this dict is used outside of trains





def main(task_args):

    logger = get_logger('train')
    logger.info("######### starting main train.py #########")
    

    ###############################################################################
    # Load data
    ###############################################################################

    # Define train, valid and test datasets
    train_loader, val_loader, test_loader, future_loader = get_data_loaders(task_args.shuffled_epochs, task_args.train, task_args.dataset_path, task_args.samples_count, task_args.batch_size, task_args.max_sequence_length, args.time_weight)

    
    return

    # setup data_loader instances
    data_loader = config.init_obj('data_loader', module_data)
    valid_data_loader = data_loader.split_validation()

    # build model architecture, then print to console
    model = config.init_obj('arch', module_arch)
    logger.info(model)

    # prepare for (multi-device) GPU training
    device, device_ids = prepare_device(config['n_gpu'])
    model = model.to(device)
    if len(device_ids) > 1:
        model = torch.nn.DataParallel(model, device_ids=device_ids)

    # get function handles of loss and metrics
    criterion = getattr(module_loss, config['loss'])
    metrics = [getattr(module_metric, met) for met in config['metrics']]

    # build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = config.init_obj('optimizer', torch.optim, trainable_params)
    lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)

    trainer = Trainer(model, criterion, metrics, optimizer,
                      config=config,
                      device=device,
                      data_loader=data_loader,
                      valid_data_loader=valid_data_loader,
                      lr_scheduler=lr_scheduler)

    trainer.train()


if __name__ == '__main__':
    # init arg parse
    parser = argparse.ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
    
    # template args
    parser.add_argument('-r', '--resume', default=None, type=str,
                      help='path to latest checkpoint (default: None)')
    parser.add_argument('-d', '--device', default=None, type=str,
                      help='indices of GPUs to enable (default: all)')

    # more args 
    parser.add_argument("-e", "--epochs", required=False, type=int, default=10, help="number of epochs to train")
    parser.add_argument("-sc", "--samples_count", required=False, type=int, default=0,
                        help="amount of syscall sequences for training")  # 1000
    parser.add_argument("-dp", "--dataset_path", required=True, type=str,
                        help="dataset directory path for scnn (with train, valid and test folders inside)")
    parser.add_argument("-bs", "--batch_size", required=False, type=int, default=10,
                        help="amount of syscalls for each batch")
    parser.add_argument("-msl", "--max_sequence_length", required=False, type=int, default=1000,  # TODO: change maxlen to the syscall_mapping1_fix_Max1000.pickle
                        help="maximum syscall sequence length")
    parser.add_argument("-dt", "--dataset_type", required=False, type=str, default="tensor_csv",  # TODO: delete!
                        help="type of the dataset to load for scnn (binee, random, cuckoo, seq)")
    parser.add_argument("-esp", "--early_stopping_patience", required=False, type=int, default=-1,
                        help="patience for the early stopping of the training phase, default is epoch_count (won't stop)")
    parser.add_argument("-mf", "--max_fpr", required=False, type=float, default=0.02,
                        help="fpr threshold for early stopping of the training phase (float, not percentage)")
    parser.add_argument("-esm", "--early_stopping_metric", required=False, type=str, default='accuracy',
                        help="metric to consider for the early stopping of the training phase")
    parser.add_argument("-lrsm", "--LR_scheduler_metric", required=False, type=str, default='ROC_AUC',
                        help="metric to consider for the early stopping of the training phase")
    parser.add_argument("-wd", "--weight_decay", required=False, type=float, default=0.01,
                        help="learning rate of the model")
    parser.add_argument("-lr", "--learning_rate", required=False, type=float, default=1e-5,
                        help="learning rate of the model")
    parser.add_argument('--warmup_iterations', type=int, default=5000,
                        help='Number of iteration for warmup period (until reaching base learning rate)')
    parser.add_argument('--log_interval', type=int, default=100,
                        help='how many batches to wait before logging training status')
    parser.add_argument('-i', '--input_checkpoint', type=str, default='',
                        help='Loading model, optimizer, and LR-Scheduler from checkpoint.')
    parser.add_argument("--output_dir", type=str, default=output_dir,
                        help="output directory for saving model checkpoints and tensorboard_logs")
    parser.add_argument("--momentum", type=float, default=0.9,
                        help="momentum for optimizer")

    parser.add_argument("-opt", "--optimize_params", action='store_true', required=False, default=False,
                        help="optimize-params with optuna")
    parser.add_argument("-prun", "--opt_pruning", action='store_true', required=False, default=False,
                        help="enable trials pruning with optuna")
    parser.add_argument('-trials', '--opt_trials', type=int, default=None,
                        help='number of trials to optimize-params with optuna')
    parser.add_argument('-timeout', '--opt_timeout', type=float, default=None,
                        help='number of seconds to optimize-params with optuna; None=until ^C or signal.')

    parser.add_argument('-emb', '--embed_size', type=int, default=configuration_data['embed_size'],
                        help='NN embed_size')
    parser.add_argument('-hid', '--hidden_size', type=int, default=configuration_data['hidden_size'],
                        help='NN hidden_size')

    parser.add_argument('-tw', '--time_weight', type=float, default=0,
                        help='time_weight for the samples weight')

    # TODO: lookout from the similiar args - train/trains
    parser.add_argument("-trains", "--allegro_trains", action='store_true', required=False, default=False,
                        help="ATTENTION: this uses allegro-trains server (defaults to upload code to demo server!! unless local-server configured)")

    parser.add_argument('-t', "--train", type=str, default='standard',
                        choices=['standard', 'retrain', 'double_retrain', 'all'],
                        help=("train: train on train-set, valid on valid-set, test on test-set. Do test also on future-set." + \
                             "retrain: train on (train-set+valid-set), valid on test-set. Do test on future-set." + \
                             "double-retrain: train on (train-set+valid-set+test-set). Do test on future-set." + \
                              "all: running 3 executions - train + retrain + double_retrain."))

    # parser.add_argument("-checkpoint", "--save_checkpoints", action='store_true', required=False, default=False,
    #                     help="save model+optimizer+LR_scheduler checkpoints (each epoch store 12MB file to disk)")

    parser.add_argument('-checkpoint', "--save_checkpoints", type=str, default='none',
                        choices=['none', 'all', 'lr'],
                        help=("none: don't save any state each epoch." + \
                             "all: save model+optimizer+LR_scheduler checkpoints (each epoch store 12MB file to disk)." + \
                             "lr: save LR_scheduler checkpoints (each epoch store 12MB file to disk)"))

    parser.add_argument("-shuffle", "--shuffled_epochs", action='store_true', required=False, default=False,
                        help="set DataLoader argument shuffle=True, used for shuffled_epochs in which the interior order of samples inside the set is random and not time sorted.")

    
    args = parser.parse_args()

    # init stuf
    # set_max_fpr(args.max_fpr)
    output_dir = args.output_dir
    log_dir = os.path.join(args.output_dir, 'tensorboard_logs')
    checkpoints_dir = os.path.join(args.output_dir, 'checkpoints')
    if not os.path.exists(args.output_dir):
        mkdir(args.output_dir)

    # if allergro-Trains:
    if args.allegro_trains:
        from trains import Task
        task = Task.init(project_name='SCNN with TRAINS, Ignite and TensorBoard',  # Task.create
                         task_name=('Train SCNN with tensor_csv dataset: ' + task_time),
                         output_uri=output_dir)  # output_'+task_time
        configuration_data = task.connect_configuration(configuration_data)

    start_time = time.time()
    if args.optimize_params:
        optimized_study = optimize_study(args)
        joblib.dump(optimized_study, os.path.join(args.output_dir, 'optimized_study.pkl'))
    else:
        # in case of args.train == all -> run the args 3 times: train + retrain + double_retrain
        if args.train == 'all':
            procedure_args = deepcopy(args)
            for procedure in ['standard', 'retrain', 'double_retrain']:
                procedure_args.output_dir = os.path.join(args.output_dir, procedure)
                procedure_args.train = procedure
                print("starting train-procedure: " + procedure)
                print("procedure_args: " + str(procedure_args))
                # run(procedure_args)
                main(procedure_args)
        else:
            # run(args)
            main(args)
    print("--- that took %s seconds ---" % (time.time() - start_time))

    # # TODO: uncomment shutdown!!!!!!
    # import os
    # os.system("shutdown now -h")
