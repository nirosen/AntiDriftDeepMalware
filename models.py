import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch
import numpy as np


# TODO: add other models: transformers / LSTM / BERT / any language model

# sample model:

class LR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruLR, self).__init__()
        self.retry = 3
        self.linear2 = nn.Linear(1000, 2) # TODO: make TFIDF vector size generic!

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                predicted_log_probabilities2 = self.linear2(input_sequence.data.float())
                predicted_log_probabilities2_softmax = F.log_softmax(predicted_log_probabilities2, dim=1)
                #predicted_log_probabilities2_logsigmoid = F.logsigmoid(predicted_log_probabilities2)
                return predicted_log_probabilities2_softmax
            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# TODO: embedding with avg_pool

class EmbedAvgPoolLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(EmbedAvgPoolLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.embed_size, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # # input_sequence is already padded to MaxSeqLen = 1000
                # embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                #                                                 seq_lengths,
                #                                                 batch_first=True,
                #                                                 enforce_sorted=False)

                # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                # self.gru.flatten_parameters()
                # gru_out, hidden = self.gru(embedded_packed, hidden)  # TODO: delete _ unused hidden from GRU output!

                # gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                # gru_out = self.layer_norm(gru_out)

                # gru_out = gru_out.permute(1,2,0)


                #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                embedded = embedded.permute(0,2,1)
                pooled = F.max_pool1d(embedded, embedded.size(2)).squeeze(2)
                #print(gru_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)


# Vanila RNN models:

class BiGruLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                gru_out, hidden = self.gru(embedded_packed, hidden)  # TODO: delete _ unused hidden from GRU output!

                gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                gru_out = self.layer_norm(gru_out)

                gru_out = gru_out.permute(1,2,0)


                #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)
                #print(gru_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

class BiLstmLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiLstmLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.LSTM = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.LSTM.flatten_parameters()
                # TODO: If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
                lstm_out, (hn, cn) = self.LSTM(embedded_packed)  # TODO: delete _ unused hidden (hn, cn) 

                lstm_out, _ = utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)

                lstm_out = self.layer_norm(lstm_out)

                lstm_out = lstm_out.permute(1,2,0)


                #lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()
                pooled = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)
                #print(lstm_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

class CnnLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(CnnLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.kernels = [3,4,5]
        self.channels = self.embed_size
        # Specify convolutions with filters of different sizes (fs)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels=self.embed_size,
                                              out_channels=self.channels,
                                              kernel_size=k)
                                    for k in self.kernels])
        # self.layer_norm = nn.LayerNorm(len(self.kernels) * self.channels)
        self.linear = nn.Linear(len(self.kernels) * self.channels, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded).permute(0, 2, 1)
                
                #Perform convolutions and apply activation functions
                #convolution and global max pooling
                conveds = [conv(embedded)
                            for conv in self.convs]

                # pooleds = [F.avg_pool1d(conved[:,:,:input_sequence.shape[1]], input_sequence.shape[1]).squeeze(2)
                #           for conved in conveds]

                # doing manual max pool since F.avg_pool1d and F.max_pool1d did not reduce dimension. need to check this.
                pooleds = [conved.permute(0, 2, 1).max(1)[0]
                            for conved in conveds]
                
                relued = [F.relu(pooled)
                          for pooled in pooleds]

                out = F.dropout(torch.cat(relued, dim=1), p=0.5)
                
                return F.log_softmax(self.linear(out))

                # embedded = self.embedding(input_sequence_padded)

                # # input_sequence is already padded to MaxSeqLen = 1000
                # embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                #                                                 seq_lengths,
                #                                                 batch_first=True,
                #                                                 enforce_sorted=False)

                # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                # self.LSTM.flatten_parameters()
                # # TODO: If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
                # lstm_out, (hn, cn) = self.LSTM(embedded_packed)  # TODO: delete _ unused hidden (hn, cn) 

                # lstm_out, _ = utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)

                # lstm_out = self.layer_norm(lstm_out)

                # lstm_out = lstm_out.permute(1,2,0)

                # #lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()
                # pooled = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)
                # #print(lstm_out.size())
                # relued = F.relu(pooled)
                # out = F.dropout(relued, p=0.5)
                # return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# TODO: bug max instead of avg!!!
class BiGruAvgPoolLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruAvgPoolLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                gru_out, hidden = self.gru(embedded_packed, hidden)  # TODO: delete _ unused hidden from GRU output!

                gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                gru_out = self.layer_norm(gru_out)

                gru_out = gru_out.permute(1,2,0)


                #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)
                #print(gru_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

class BiLstmAvgPoolLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiLstmAvgPoolLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.LSTM = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.LSTM.flatten_parameters()
                # TODO: If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
                lstm_out, (hn, cn) = self.LSTM(embedded_packed)  # TODO: delete _ unused hidden (hn, cn) 

                lstm_out, _ = utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)

                lstm_out = self.layer_norm(lstm_out)

                lstm_out = lstm_out.permute(1,2,0)


                #lstm_out = torch.transpose(lstm_out, 1, 2).contiguous()
                pooled = F.avg_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)
                #print(lstm_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

class BiGruLastPoolLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruLastPoolLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                _, hidden = self.gru(embedded_packed, hidden)  # TODO: delete _ unused hidden from GRU output!

                # hidden.shape = (n_layers * n_directions, batch_size, hidden_dim)
                hidden = hidden.view(1, 2, input_sequence.shape[0], self.hidden_size)
                # This view() comes directly from the PyTorch docs
                # hidden.shape = (n_layers, n_directions, batch_size, hidden_dim)
                hidden = hidden[-1]
                # hidden.shape = (n_directions, batch_size, hidden_dim)
                hidden_forward, hidden_backward = hidden[0], hidden[1]
                # Both shapes (batch_size, hidden_dim)
                pooled = torch.cat((hidden_forward, hidden_backward), dim=1)
                # pooled.shape = (batch_size, 2*hidden_dim)

                # hidden = self.layer_norm(pooled)  # nir not adding this as I didnt add this in gru+attention

                relued = F.relu(pooled)
                fc_input = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(fc_input), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

class BiLstmLastPoolLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiLstmLastPoolLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.LSTM = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.LSTM.flatten_parameters()
                # TODO: If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
                _, (hn, cn) = self.LSTM(embedded_packed)  # TODO: delete _ unused hidden (hn, cn) 
                # hn containing the hidden state for `t = seq_len`.
                # cn containing the cell state for `t = seq_len`
                hidden = hn  # TODO: check 

                # hidden.shape = (n_layers * n_directions, batch_size, hidden_dim)
                hidden = hidden.view(1, 2, input_sequence.shape[0], self.hidden_size)
                # This view() comes directly from the PyTorch docs
                # hidden.shape = (n_layers, n_directions, batch_size, hidden_dim)
                hidden = hidden[-1]
                # hidden.shape = (n_directions, batch_size, hidden_dim)
                hidden_forward, hidden_backward = hidden[0], hidden[1]
                # Both shapes (batch_size, hidden_dim)
                pooled = torch.cat((hidden_forward, hidden_backward), dim=1)
                # pooled.shape = (batch_size, 2*hidden_dim)

                # hidden = self.layer_norm(pooled)  # nir not adding this as I didnt add this in gru+attention

                relued = F.relu(pooled)
                fc_input = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(fc_input), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)


# Attention and Transformers models:


# 1. attention class

class AttentionModel:
    NONE = 0
    DOT = 1
    GENERAL = 2  # TODO: this has a bug yet!
class Attention(nn.Module):
    def __init__(self, device, method, hidden_size):
        super(Attention, self).__init__()
        self.device = device

        self.method = method
        self.hidden_size = hidden_size

        self.concat_linear = nn.Linear(self.hidden_size * 2, self.hidden_size)

        if self.method == AttentionModel.GENERAL:
            self.attn = nn.Linear(self.hidden_size, hidden_size)  # TODO: this has a bug yet!

    def forward(self, rnn_outputs, final_hidden_state):
        # rnn_output.shape:         (batch_size, seq_len, hidden_size)
        # final_hidden_state.shape: (batch_size, hidden_size)
        # NOTE: hidden_size may also reflect bidirectional hidden states (hidden_size = num_directions * hidden_dim)
        batch_size, seq_len, _ = rnn_outputs.shape  # 600 10 1000
        if self.method == AttentionModel.DOT:
            attn_weights = torch.bmm(rnn_outputs, final_hidden_state.unsqueeze(2))
        elif self.method == AttentionModel.GENERAL:
            attn_weights = self.attn(rnn_outputs) # (batch_size, seq_len, hidden_dim)
            attn_weights = torch.bmm(attn_weights, final_hidden_state.unsqueeze(2))
        else:
            raise Exception("[Error] Unknown AttentionModel.")

        attn_weights = F.softmax(attn_weights.squeeze(2), dim=1)

        context = torch.bmm(rnn_outputs.transpose(1, 2), attn_weights.unsqueeze(2)).squeeze(2)

        attn_hidden = torch.tanh(self.concat_linear(torch.cat((context, final_hidden_state), dim=1)))

        return attn_hidden, attn_weights

# https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/models/text/classifier/rnn.py
class BiGruAttnLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruAttnLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.attn = Attention(self.device, 1, self.hidden_size * 2)  # 1=DOT 2=GENERAL
        
        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                gru_out, hidden = self.gru(embedded_packed, hidden)

                gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                # gru_out = self.layer_norm(gru_out)  # TODO: do I need norm before attention?
                # gru_out = gru_out.permute(1,2,0)

                # Extract last hidden state
                final_state = hidden.view(1, 2, input_sequence.shape[0], self.hidden_size)[-1]

                # Handle 2 directions
                h_1, h_2 = final_state[0], final_state[1]
                final_hidden_state = h_1 + h_2               # Add both states (requires changes to the input size of first linear layer + attention layer)
                final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states

                # Push through attention layer
                attn_weights = None
                rnn_output = gru_out.permute(1, 0, 2)  # need batch_size, seq_len, _   was 1000, 10, 600
                X, attn_weights = self.attn(rnn_output, final_hidden_state)

                # #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                # pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)  # TODO: do I need max_pool1d after attention?
                # #print(gru_out.size())
                # relued = F.relu(pooled)
                # out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(X), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# https://github.com/chrisvdweth/ml-toolkit/blob/master/pytorch/models/text/classifier/rnn.py
class BiLstmAttnLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiLstmAttnLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.LSTM = nn.LSTM(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.attn = Attention(self.device, 1, self.hidden_size * 2)  # 1=DOT 2=GENERAL
        
        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.LSTM.flatten_parameters()
                # TODO: If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
                lstm_out, (hn, cn) = self.LSTM(embedded_packed)  # TODO: delete _ unused hidden (hn, cn) 

                lstm_out, _ = utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)

                # lstm_out = self.layer_norm(lstm_out)  # TODO: do I need norm before attention?
                # lstm_out = lstm_out.permute(1,2,0)


                # Extract last hidden state
                final_state = hidden.view(1, 2, input_sequence.shape[0], self.hidden_size)[-1]

                # Handle 2 directions
                h_1, h_2 = final_state[0], final_state[1]
                final_hidden_state = h_1 + h_2               # Add both states (requires changes to the input size of first linear layer + attention layer)
                final_hidden_state = torch.cat((h_1, h_2), 1)  # Concatenate both states

                # Push through attention layer
                attn_weights = None
                rnn_output = lstm_out.permute(1, 0, 2)  # need batch_size, seq_len, _   was 1000, 10, 600
                X, attn_weights = self.attn(rnn_output, final_hidden_state)

                return F.log_softmax(self.linear(X), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)



# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
def create_sinusoidal_embeddings(embeds):
    position_enc = torch.tensor([
        [pos / np.power(10000, 2 * (j // 2) / embeds.embedding_dim) for j in range(embeds.embedding_dim)]
                                                                    for pos in range(embeds.num_embeddings)])
    embeds.weight[:, 0::2] = torch.sin(position_enc[:, 0::2])
    embeds.weight[:, 1::2] = torch.cos(position_enc[:, 1::2])
    embeds.weight.detach_()
    embeds.weight.requires_grad = False

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
class BiGruGptCls(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(BiGruGptCls, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 100#96#100 300 80
        self.hidden_size = 150#144#150 300 100
        self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        self.attentions_out_size = self.gru_out_size
        self.MaxSeqLen = MaxSeqLen + 1 # one for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 2 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)

        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # Notive: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 1 # 3  # TODO Liron: transformer must have >1  MultiheadAttention-layers and no other layer (execpt linear ones)
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.gru_out_size, num_heads=5, dropout=0.1))  # num_heads=3
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.gru_out_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.gru_out_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))

        self.linear = nn.Linear(self.attentions_out_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?

            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 1:input_sequence.shape[1]+1] = input_sequence.long()

            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)

            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)

            # run gru on packed embedded_seq (h)
            h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            self.gru.flatten_parameters()
            gru_out, _ = self.gru(h_packed, hidden)
            gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            gru_out = gru_out.permute(1, 0, 2)

            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)

            h = gru_out

            h = self.dropout(h)

            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            transformer_out = torch.transpose(h, 1, 2).contiguous()
            transformer_out_first_layer = transformer_out[:, :, 0]
            relued = F.relu(transformer_out_first_layer)
            dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
class GPT2CLS(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(GPT2CLS, self).__init__()
        # commented out the GRU part:
        #self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        #self.attentions_out_size = self.gru_out_size
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 100#192#300 80
        self.hidden_size = 150#288#300 100
        self.MaxSeqLen = MaxSeqLen + 1 # one for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 2 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)
        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False  # TODO: check this as arg!
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # Notive: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 6
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.embed_size, num_heads=10, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.embed_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.embed_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.embed_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.embed_size, eps=1e-12))

        self.linear = nn.Linear(self.embed_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?
            # print('1')
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 1:input_sequence.shape[1]+1] = input_sequence.long()
            # print('2')
            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)
            # print('3')
            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            # print('4')
            # TODO: delete block
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)
            # print('5')
            # TODO: commented out GRU part:
            #
            # # run gru on packed embedded_seq (h)
            # h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h_packed, hidden)
            # gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            # gru_out = gru_out.permute(1, 0, 2)
            #
            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)
            #
            #h = gru_out

            h = self.dropout(h)
            # print('6')
            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            # print('7')
            transformer_out = torch.transpose(h, 1, 2).contiguous()

            transformer_out_first_layer = transformer_out[:, :, 0]
            # unsesed, delete:
            #relued = F.relu(transformer_out_first_layer)
            #dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            # print('8')
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.pys
class GPT2(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(GPT2, self).__init__()
        # commented out the GRU part:
        #self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        #self.attentions_out_size = self.gru_out_size
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 192#300 80
        self.hidden_size = 288#300 100
        self.MaxSeqLen = MaxSeqLen  # +1 for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 1 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)
        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False  # TODO: check this as arg!
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # TODO: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 6
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.embed_size, num_heads=6, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.embed_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.embed_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.embed_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.embed_size, eps=1e-12))

        self.linear = nn.Linear(self.embed_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            # cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?
            # print('1')
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            #input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 0:input_sequence.shape[1]] = input_sequence.long()
            # print('2')
            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)
            # print('3')
            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            # print('4')
            # TODO: delete block
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)
            # print('5')
            # TODO: commented out GRU part:
            #
            # # run gru on packed embedded_seq (h)
            # h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h_packed, hidden)
            # gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            # gru_out = gru_out.permute(1, 0, 2)
            #
            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)
            #
            #h = gru_out

            h = self.dropout(h)
            # print('6')
            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            # print('7')
            transformer_out = torch.transpose(h, 1, 2).contiguous()

            transformer_out_first_layer = transformer_out[:, :, 0]
            # unsesed, delete:
            #relued = F.relu(transformer_out_first_layer)
            #dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            # print('8')
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
















# import torch.nn as nn
# import torch.nn.functional as F
# from base import BaseModel


# class MnistModel(BaseModel):
#     def __init__(self, num_classes=10):
#         super().__init__()
#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
#         self.conv2_drop = nn.Dropout2d()
#         self.fc1 = nn.Linear(320, 50)
#         self.fc2 = nn.Linear(50, num_classes)

#     def forward(self, x):
#         x = F.relu(F.max_pool2d(self.conv1(x), 2))
#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
#         x = x.view(-1, 320)
#         x = F.relu(self.fc1(x))
#         x = F.dropout(x, training=self.training)
#         x = self.fc2(x)
#         return F.log_softmax(x, dim=1)
