import torch.nn as nn
import torch.nn.utils as utils
import torch.nn.functional as F
import torch
import numpy as np


# TODO: add other models: transformers / LSTM / BERT / any language model

# sample model:

class LR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruLR, self).__init__()
        self.retry = 3
        self.linear2 = nn.Linear(1000, 2) # TODO: make TFIDF vector size generic!

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                predicted_log_probabilities2 = self.linear2(input_sequence.data.float())
                predicted_log_probabilities2_softmax = F.log_softmax(predicted_log_probabilities2, dim=1)
                #predicted_log_probabilities2_logsigmoid = F.logsigmoid(predicted_log_probabilities2)
                return predicted_log_probabilities2_softmax
            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# RNN models:

class BiGruLR(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, num_classes, categories_count, embed_size, hidden_size):
        super(BiGruLR, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.categories_count = categories_count
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.MaxSeqLen = MaxSeqLen

        # num_embeddings = Vocab-Size = categories_count = 48
        self.embedding = nn.Embedding(num_embeddings=self.categories_count+1, embedding_dim=self.embed_size)  # TODO: change embedding according to Vocab-Size!!

        #self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))
        self.embedding.weight.requires_grad = False

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.layer_norm = nn.LayerNorm(self.hidden_size*2)

        self.linear = nn.Linear(self.hidden_size*2, num_classes) # bidirectional implies multiply by 2

        self.retry = 3

    def init_hidden(self, num_layers=1, batch_size=10):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None):
        for try_count in range(self.retry):
            try:
                # padding each seq with zeros until max_seq_len = 10,000
                input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
                input_sequence_padded[:, :input_sequence.shape[1]] = input_sequence.long()

                embedded = self.embedding(input_sequence_padded)

                # input_sequence is already padded to MaxSeqLen = 1000
                embedded_packed = utils.rnn.pack_padded_sequence(embedded,
                                                                seq_lengths,
                                                                batch_first=True,
                                                                enforce_sorted=False)

                hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
                self.gru.flatten_parameters()
                gru_out, hidden = self.gru(embedded_packed, hidden)

                gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)

                gru_out = self.layer_norm(gru_out)

                gru_out = gru_out.permute(1,2,0)


                #gru_out = torch.transpose(gru_out, 1, 2).contiguous()
                pooled = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)
                #print(gru_out.size())
                relued = F.relu(pooled)
                out = F.dropout(relued, p=0.5)

                return F.log_softmax(self.linear(out), dim=1)

            except Exception as ex:
                print(ex)
                print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# Attention and Transformers models:

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
def create_sinusoidal_embeddings(embeds):
    position_enc = torch.tensor([
        [pos / np.power(10000, 2 * (j // 2) / embeds.embedding_dim) for j in range(embeds.embedding_dim)]
                                                                    for pos in range(embeds.num_embeddings)])
    embeds.weight[:, 0::2] = torch.sin(position_enc[:, 0::2])
    embeds.weight[:, 1::2] = torch.cos(position_enc[:, 1::2])
    embeds.weight.detach_()
    embeds.weight.requires_grad = False

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
class BiGruGptCls(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(BiGruGptCls, self).__init__()
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 96#100 300 80
        self.hidden_size = 144#150 300 100
        self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        self.attentions_out_size = self.gru_out_size
        self.MaxSeqLen = MaxSeqLen + 1 # one for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 2 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)

        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.gru = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size,
                          dropout=0.5, num_layers=1, bidirectional=True, batch_first=True)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # Notive: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 3  # TODO Liron: transformer must have >1  MultiheadAttention-layers and no other layer (execpt linear ones)
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.gru_out_size, num_heads=3, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.gru_out_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.gru_out_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.gru_out_size, eps=1e-12))

        self.linear = nn.Linear(self.attentions_out_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?

            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 1:input_sequence.shape[1]+1] = input_sequence.long()

            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)

            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)

            # run gru on packed embedded_seq (h)
            h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            self.gru.flatten_parameters()
            gru_out, _ = self.gru(h_packed, hidden)
            gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            gru_out = gru_out.permute(1, 0, 2)

            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)

            h = gru_out

            h = self.dropout(h)

            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            transformer_out = torch.transpose(h, 1, 2).contiguous()
            transformer_out_first_layer = transformer_out[:, :, 0]
            relued = F.relu(transformer_out_first_layer)
            dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.py
class GPT2CLS(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(GPT2CLS, self).__init__()
        # commented out the GRU part:
        #self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        #self.attentions_out_size = self.gru_out_size
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 100#192#300 80
        self.hidden_size = 150#288#300 100
        self.MaxSeqLen = MaxSeqLen + 1 # one for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 2 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)
        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False  # TODO: check this as arg!
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # Notive: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 6
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.embed_size, num_heads=10, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.embed_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.embed_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.embed_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.embed_size, eps=1e-12))

        self.linear = nn.Linear(self.embed_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?
            # print('1')
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 1:input_sequence.shape[1]+1] = input_sequence.long()
            # print('2')
            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)
            # print('3')
            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            # print('4')
            # TODO: delete block
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)
            # print('5')
            # TODO: commented out GRU part:
            #
            # # run gru on packed embedded_seq (h)
            # h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h_packed, hidden)
            # gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            # gru_out = gru_out.permute(1, 0, 2)
            #
            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)
            #
            #h = gru_out

            h = self.dropout(h)
            # print('6')
            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            # print('7')
            transformer_out = torch.transpose(h, 1, 2).contiguous()

            transformer_out_first_layer = transformer_out[:, :, 0]
            # unsesed, delete:
            #relued = F.relu(transformer_out_first_layer)
            #dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            # print('8')
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)

# https://github.com/huggingface/naacl_transfer_learning_tutorial/blob/master/pretraining_model.pys
class GPT2(nn.Module):
    def __init__(self, device, batch_size, MaxSeqLen, categories_count, embedding_size, hidden_size, classes_count, normalize=False):
        super(GPT2, self).__init__()
        # commented out the GRU part:
        #self.gru_out_size = self.hidden_size*2 # biGru implies hidden_size*2
        #self.attentions_out_size = self.gru_out_size
        self.device = device
        self.batch_size = batch_size
        self.embed_size = 192#300 80
        self.hidden_size = 288#300 100
        self.MaxSeqLen = MaxSeqLen  # +1 for cls token at the start of each seq
        self.causal = False
        self.categories_count = categories_count
        num_of_ids = self.categories_count + 1 # VocabSize=45[1..45]; padding=1[0]; cls=1[100] should i change cls value?
        self.tokens_embeddings = nn.Embedding(num_embeddings=num_of_ids, embedding_dim=self.embed_size)
        # Notice: i'm not doing the position_embeddings on the sequence
        num_max_positions = self.MaxSeqLen # help: Max input length
        self.position_embeddings = nn.Embedding(num_embeddings=num_max_positions, embedding_dim=self.embed_size)

        sinusoidal_embeddings = False  # TODO: check this as arg!
        if sinusoidal_embeddings:
            create_sinusoidal_embeddings(self.position_embeddings)

        self.dropout = nn.Dropout(0.1)

        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()
        self.layer_norms_1, self.layer_norms_2 = nn.ModuleList(), nn.ModuleList()

        # TODO: my GPU 8GB memory can't handle more than 1 layer with self.embed_size=300
        num_layers = 6
        for _ in range(num_layers):
            self.attentions.append(nn.MultiheadAttention(embed_dim=self.embed_size, num_heads=6, dropout=0.1))
            self.feed_forwards.append(nn.Sequential(nn.Linear(self.embed_size, self.hidden_size),
                                                    nn.ReLU(),
                                                    nn.Linear(self.hidden_size, self.embed_size)))
            self.layer_norms_1.append(nn.LayerNorm(self.embed_size, eps=1e-12))
            self.layer_norms_2.append(nn.LayerNorm(self.embed_size, eps=1e-12))

        self.linear = nn.Linear(self.embed_size, 2)

    def init_hidden(self, batch_size, num_layers=1):
        return torch.zeros(num_layers * 2, batch_size, self.hidden_size)

    def forward(self, input_sequence, seq_lengths, hn=None, padding_mask=None):
        try:
            # cls = self.categories_count + 1 # 30APR21 45+1 worked, 46+1 didnt work; VocabSize=45[1..45]; padding[0]; cls=[46] should i change cls value?
            # print('1')
            # padding each seq with zeros until max_seq_len = 10,000
            input_sequence_padded = torch.zeros(input_sequence.shape[0], self.MaxSeqLen).long().to(self.device)
            # adding first word as cls
            #input_sequence_padded[:, 0:1] = (cls*torch.ones(1)).cuda().long()
            input_sequence_padded[:, 0:input_sequence.shape[1]] = input_sequence.long()
            # print('2')
            ### positions
            positions = torch.arange(len(input_sequence_padded), device=input_sequence_padded.device).unsqueeze(-1)
            # print('3')
            ### tokens
            h = self.tokens_embeddings(input_sequence_padded)
            # print('4')
            # TODO: delete block
            ## packing the seq destroyed my results...
            # input_sequence_packed = utils.rnn.pack_padded_sequence(input_sequence_padded,
            #                                                        seq_lengths,
            #                                                        batch_first=True,
            #                                                        enforce_sorted=False)
            # # Notice: i'm not doing the position_embeddings on the sequence
            # h = self.tokens_embeddings(input_sequence_packed.data)
            # h, _ = utils.rnn.pad_packed_sequence(utils.rnn.PackedSequence(h, input_sequence_packed.batch_sizes),
            #                                      batch_first=False)
            # h = h.permute(1, 0, 2)

            # add pos+tok
            h = h + self.position_embeddings(positions).expand_as(h)
            # print('5')
            # TODO: commented out GRU part:
            #
            # # run gru on packed embedded_seq (h)
            # h_packed = utils.rnn.pack_padded_sequence(h, seq_lengths, batch_first=True, enforce_sorted=False)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]), requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h_packed, hidden)
            # gru_out, _ = utils.rnn.pad_packed_sequence(gru_out, batch_first=False)
            # gru_out = gru_out.permute(1, 0, 2)
            #
            # # run gru on embedded_seq (h)
            # hidden = nn.Parameter(self.init_hidden(num_layers=1, batch_size=input_sequence.shape[0]),
            #                       requires_grad=True).cuda()
            # self.gru.flatten_parameters()
            # gru_out, _ = self.gru(h, hidden)
            #
            #h = gru_out

            h = self.dropout(h)
            # print('6')
            # set x to unpacked input
            x = input_sequence_padded

            attn_mask = None
            if self.causal:
                attn_mask = torch.full((len(x), len(x)), -float('Inf'), device=h.device, dtype=h.dtype)
                attn_mask = torch.triu(attn_mask, diagonal=1)

            for layer_norm_1, attention, layer_norm_2, feed_forward in zip(self.layer_norms_1, self.attentions,
                                                                           self.layer_norms_2, self.feed_forwards):
                h = layer_norm_1(h)
                x, _ = attention(h, h, h, attn_mask=attn_mask, need_weights=False, key_padding_mask=padding_mask)
                x = self.dropout(x)
                h = x + h

                h = layer_norm_2(h)
                x = feed_forward(h)
                x = self.dropout(x)
                h = x + h

            # print('7')
            transformer_out = torch.transpose(h, 1, 2).contiguous()

            transformer_out_first_layer = transformer_out[:, :, 0]
            # unsesed, delete:
            #relued = F.relu(transformer_out_first_layer)
            #dropped = F.dropout(relued, p=0.1)
            out = F.log_softmax(self.linear(transformer_out_first_layer))
            torch.cuda.empty_cache()  # clear GPU memory
            # print('8')
            return out

            #out = F.log_softmax(self.linear(transformer_out.mean(dim=2)))
            # pooled = F.max_pool1d(h_out, h_out.size(2)).squeeze(2)
            # relued = F.relu(pooled)
            # out = F.dropout(relued, p=0.1)

        except Exception as ex:
            print(ex)
            print(f"Failed to forward input_sequence of shape: {input_sequence.shape}.", ex)
















# import torch.nn as nn
# import torch.nn.functional as F
# from base import BaseModel


# class MnistModel(BaseModel):
#     def __init__(self, num_classes=10):
#         super().__init__()
#         self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
#         self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
#         self.conv2_drop = nn.Dropout2d()
#         self.fc1 = nn.Linear(320, 50)
#         self.fc2 = nn.Linear(50, num_classes)

#     def forward(self, x):
#         x = F.relu(F.max_pool2d(self.conv1(x), 2))
#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
#         x = x.view(-1, 320)
#         x = F.relu(self.fc1(x))
#         x = F.dropout(x, training=self.training)
#         x = self.fc2(x)
#         return F.log_softmax(x, dim=1)
