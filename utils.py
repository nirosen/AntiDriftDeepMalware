
import torch
import errno
import os
# import dataset
# from custom_metrics import *  # TrimmedAUC, MaxTPR, RocCurvePlot, RocCurveIterationsPlot
import pandas as pd
import pickle



def mkdir(path):
    try:
        if not os.path.exists(path):
            os.makedirs(path)
    except OSError as e:
        if e.errno != errno.EEXIST:
            raise


import sys
import logging
import logging.handlers


loggers = {}

def get_logger(name, dir='./'):
    global loggers
    if loggers.get(name):
        return loggers.get(name)
    else:
        # create logger and set root logging level
        logger = logging.getLogger(name)
        logger.setLevel(logging.DEBUG)
        # Create formatters and add it to handlers
        console_handler = logging.StreamHandler(stream=sys.stdout)
        console_handler.setLevel(logging.DEBUG)
        console_format = logging.Formatter(fmt="%(asctime)s - %(levelname)s\t \t%(name)s : [%(funcName)s]\t - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
        console_handler.setFormatter(console_format)
        # Create formatters and add it to handlers
        file_handler = logging.FileHandler(filename=os.path.join(os.path.abspath(dir), 'log_'+name+'.txt'), mode='a')
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter(fmt="%(asctime)s - %(levelname)s - %(name)s - %(pathname)s : %(funcName)s [%(lineno)d] - %(message)s",
                                        datefmt="%Y-%m-%d %H:%M:%S")
        file_handler.setFormatter(file_format)
        # Add handlers to the logger
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        # save and return
        loggers[name] = logger
        return logger







# template funcs


import torch


def prepare_device(n_gpu_use):
    """
    setup GPU device if available. get gpu device indices which are used for DataParallel
    """
    n_gpu = torch.cuda.device_count()
    if n_gpu_use > 0 and n_gpu == 0:
        print("Warning: There\'s no GPU available on this machine,"
              "training will be performed on CPU.")
        n_gpu_use = 0
    if n_gpu_use > n_gpu:
        print(f"Warning: The number of GPU\'s configured to use is {n_gpu_use}, but only {n_gpu} are "
              "available on this machine.")
        n_gpu_use = n_gpu
    device = torch.device('cuda:0' if n_gpu_use > 0 else 'cpu')
    list_ids = list(range(n_gpu_use))
    return device, list_ids
