import torch
from abc import abstractmethod
from numpy import inf

import numpy as np
import torch


from tqdm import tqdm
import math
import pandas as pd
from sklearn.metrics import roc_auc_score
from utils import mkdir, get_logger 
import os


# TODO: use the function from train.py for unified purpose
def eval_step(model, criterion, device, eval_loader, set_name):
    logger = get_logger('test')
    desc = ("Evaluating %s set" % (set_name))
    logger.info(desc)
    # Turn on training mode which enables dropout.
    model.eval()
    # train_metrics.reset()
    running_loss, running_log_preds, running_labels, running_times, running_hashes = 0, [], [], [], []
    with torch.no_grad():
        with tqdm(range(len(eval_loader)), desc=desc, total=(len(eval_loader))) as t:
            # do batches
            for batch_idx, batch in zip(t, eval_loader):
                # update prog bar
                padded_sequence_tensors, samples_labels, seq_lengths, _, samples_times, samples_hashes = batch  # batch return padded_sequence_tensor, syscalls_label, seq_length, sample_weight, self.times[item], self.hashes[item]
                labels = torch.Tensor(samples_labels).to(device).long()
                # seq_lengths = torch.Tensor(seq_lengths).to(device).long()  # shuould be on cpu from pytorch-1.7 (>1.4) bug - 'lengths' argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
                padded_sequence_tensors = torch.stack(padded_sequence_tensors).to(device)

                # forward the packed sequences batch and get the predicted log probabilities
                predicted_log_probs = model(padded_sequence_tensors, seq_lengths)  # , hn)
                pred_loss = criterion(predicted_log_probs, labels)
                scalar_loss = pred_loss.item()  # correct = predicted_log_probs.argmax(dim=1).eq(labels).sum().item()
                running_loss += scalar_loss
                avg_loss = running_loss / (batch_idx + 1)

                # save batch stats
                running_log_preds+=predicted_log_probs.tolist()
                running_labels+=samples_labels
                running_times+=samples_times
                running_hashes+=samples_hashes
                
                t.set_postfix(loss=avg_loss)
                padded_sequence_tensors = labels = seq_lengths = predicted_log_probs = pred_loss = None

    epoch_stats_df = pd.DataFrame()
    epoch_stats_df['predicted_log_probability'] = running_log_preds
    epoch_stats_df['label'] = running_labels
    epoch_stats_df['time'] = running_times
    epoch_stats_df['hash'] = running_hashes
    return avg_loss, epoch_stats_df

def save_epoch_stats(output_dir, set_name, epoch_stats_df):
    epochs_stats_dir = os.path.join(output_dir, 'epochs_stats', set_name)
    mkdir(epochs_stats_dir)
    epoch_stats_df.to_pickle(os.path.join(epochs_stats_dir, 'epoch_0_stats_pickle.gz'))

"""
Full test logic
"""
# Full test logic
def test(args, model, criterion, device, train_loader=None, val_loader=None, test_loader=None, future_loader=None, len_epoch=None):
    logger = get_logger('test')

    # do evaluation of each set
    for eval_loader, set_name in zip([train_loader, val_loader, test_loader, future_loader], ['train', 'valid', 'test', 'future']):
        # skip None sets for retrain/double-retrain procedures
        if eval_loader:
            eval_loss, epoch_stats_df = eval_step(model, criterion, device, eval_loader, set_name)
            # TODO: save epoch_stats_df to outputdir!!!
            save_epoch_stats(args.output_dir, set_name, epoch_stats_df)
            auc = roc_auc_score(epoch_stats_df['label'], epoch_stats_df.predicted_log_probability.map(lambda x: x[1]))
            logger.info("finished eval of %s-set \t auc=%.2f" %(set_name, auc))




# template imports
import argparse
import collections
import torch
import numpy as np
# import data_loader.data_loaders as module_data
# import model.loss as module_loss
# import model.metric as module_metric
import models
# from parse_config import ConfigParser
from trainer import train
from utils import prepare_device, mkdir, get_logger 
from data_loader import get_data_loaders

# fix random seeds for reproducibility
SEED = 0
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(SEED)


import logging



# external imports
import os
import time
from datetime import datetime
from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter
import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter, summary # pytorch TB logger
# import optuna
import joblib
from copy import deepcopy
# import matplotlib.pyplot as plt
from glob import glob

# local imports
# import models
# from engines import create_trainer, create_evaluator



# globals
task_time = datetime.now().strftime("%d%m%Y_%H%M%S")
output_dir = os.path.join('..','output','output_'+task_time)  # '/home/john/Desktop/git/scnn_ignite/output/output_'+task_time

# num_classes is good/bad
# categories_count is the number of uniq syscalls
# TODO: change categories_count to 46 for 60K_Max1000 or 48 without limit Max1000
configuration_data = {'num_classes': 2, 'categories_count': 46, 'embed_size': 300, 'hidden_size': 300}  # TODO: this dict is used outside of trains


def load_model(filename):
    checkpoint = torch.load(filename) 
    return checkpoint['architecture'], checkpoint['model_state_dict']




def main(args):

    logger = get_logger(name='train')

    logger.info("######### starting main test.py #########")
    logger.info("######### args: ######### \n %s" %(args))

    # Define train, valid and test datasets
    logger.info("loading data")
    train_loader, val_loader, test_loader, future_loader = get_data_loaders(args.shuffled_epochs, args.train_procedure, args.dataset_path, args.samples_count, args.batch_size, args.max_sequence_length, args.time_weight)
    
    # prepare for (multi-device) GPU training
    device, device_ids = prepare_device(1)

    if args.architecture == 'Transformer':
        logger.info("chosen architecture is BiGruGptCls")
        model = models.BiGruGptCls(device=device,
                                   batch_size=args.batch_size,
                                   MaxSeqLen=args.max_sequence_length,
                                   categories_count=configuration_data.get('categories_count'),
                                   embedding_size=args.embed_size,
                                   hidden_size=args.hidden_size,
                                   classes_count=configuration_data.get('num_classes'),
                                   normalize=False)
    else:
        logger.info("defualt architecture is GRU")
        model = models.BiGruLR(device=device,
                               batch_size=args.batch_size,
                               MaxSeqLen=args.max_sequence_length,
                               num_classes=configuration_data.get('num_classes'),
                               categories_count=configuration_data.get('categories_count'),
                               embed_size=args.embed_size,
                               hidden_size=args.hidden_size)

    # prepare for (multi-device) GPU test
    model = model.to(device)
    if len(device_ids) > 1:
        model = torch.nn.DataParallel(model, device_ids=device_ids)    
    
    if os.path.exists(args.input_checkpoint):
        loaded = load_model(args.input_checkpoint)
        if loaded:
            loaded_arch = loaded[0]
            if loaded_arch == args.architecture:
                model.load_state_dict(loaded[1])
            else:
                logger.info("loaded arch is not same as arguemt --architecture, check arg and checkpoint!")
                raise Exception("loaded arch is not same as arguemt --architecture, check arg and checkpoint!")


    # get function handles of loss and metrics
    criterion = nn.NLLLoss()

    test(args, model, criterion, device, train_loader, val_loader, test_loader, future_loader)

    logger.info("####### finished [main] train.py ! ####### ")



if __name__ == '__main__':
    # init arg parse
    parser = argparse.ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)
    
    parser.add_argument("-sc", "--samples_count", required=False, type=int, default=0,
                        help="amount of syscall sequences for training")  # 1000
    parser.add_argument("-dp", "--dataset_path", required=True, type=str,
                        help="dataset directory path for scnn (with train, valid and test folders inside)")
    parser.add_argument("-bs", "--batch_size", required=False, type=int, default=10,
                        help="amount of syscalls for each batch")
    parser.add_argument("-msl", "--max_sequence_length", required=False, type=int, default=1000,  # TODO: change maxlen to the syscall_mapping1_fix_Max1000.pickle
                        help="maximum syscall sequence length")
    parser.add_argument("-dt", "--dataset_type", required=False, type=str, default="tensor_csv",  # TODO: delete!
                        help="type of the dataset to load for scnn (binee, random, cuckoo, seq)")
    parser.add_argument('-i', '--input_checkpoint', type=str, default='',
                        help='Loading model from checkpoint.')
    parser.add_argument("--output_dir", type=str, default=output_dir,
                        help="output directory for saving model checkpoints and tensorboard_logs")
    parser.add_argument('-emb', '--embed_size', type=int, default=configuration_data['embed_size'],
                        help='NN embed_size')
    parser.add_argument('-hid', '--hidden_size', type=int, default=configuration_data['hidden_size'],
                        help='NN hidden_size')
    parser.add_argument('-tw', '--time_weight', type=float, default=0,
                        help='time_weight for the samples weight')
    parser.add_argument("-trains", "--allegro_trains", action='store_true', required=False, default=False,
                        help="ATTENTION: this uses allegro-trains server (defaults to upload code to demo server!! unless local-server configured)")
    parser.add_argument('-t', "--train_procedure", type=str, default='train',
                        choices=['train', 'retrain', 'double_retrain', 'all'],
                        help=("train: train on train-set, valid on valid-set, test on test-set. Do test also on future-set." + \
                             "retrain: train on (train-set+valid-set), valid on test-set. Do test on future-set." + \
                             "double-retrain: train on (train-set+valid-set+test-set). Do test on future-set." + \
                              "all: running 3 executions - train + retrain + double_retrain."))
    parser.add_argument("-shuffle", "--shuffled_epochs", action='store_true', required=False, default=False,
                        help="set DataLoader argument shuffle=True, used for shuffled_epochs in which the interior order of samples inside the set is random and not time sorted.")
    parser.add_argument('-arch', "--architecture", type=str, default='GRU',
                    choices=['GRU', 'Transformer'],
                    help=("model architecture: 'GRU', 'Transformer'."))

    args = parser.parse_args()

    # init stuf
    output_dir = args.output_dir
    mkdir(args.output_dir)

    # if allergro-Trains:
    if args.allegro_trains:
        from trains import Task
        task = Task.init(project_name='SCNN with TRAINS, Ignite and TensorBoard',  # Task.create
                         task_name=('Train SCNN with tensor_csv dataset: ' + task_time),
                         output_uri=output_dir) 
        configuration_data = task.connect_configuration(configuration_data)

    start_time = time.time()
    logger = get_logger(name='test', dir=args.output_dir)
    # in case of args.train == all -> run the args 3 times: train + retrain + double_retrain
    if args.train_procedure == 'all':
        logger.info("\n\n\n ### running all training-procedures: ###\n\n\n")
        for procedure in ['train', 'retrain', 'double_retrain']:
            procedure_args = deepcopy(args)
            procedure_args.train_procedure = procedure                
            # create procedure dir
            procedure_args.output_dir = os.path.join(args.output_dir, procedure)
            mkdir(procedure_args.output_dir)
            # run procedurrre
            logger.info("\n\n\n ### starting train-procedure: %s ###\n\n\n" %(procedure))
            procedure_start_time = time.time()
            main(procedure_args)
            logger.info("\n\n\n ### train-procedure: %s finished in %d seconds ###\n\n\n" % (procedure, time.time() - procedure_start_time))
    else:
        main(args)
    logger.info("--- that took %s seconds ---" % (time.time() - start_time))


